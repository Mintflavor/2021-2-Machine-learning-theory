{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_engine.outliers import OutlierTrimmer\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "import keras.metrics as metrics\n",
    "import keras.regularizers as regularizers\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, pred):\n",
    "    pred_b = (pred > 0.5)\n",
    "    acc = accuracy_score(y_test, pred_b)\n",
    "    pre = precision_score(y_test, pred_b, pos_label=1)\n",
    "    re = recall_score(y_test, pred_b, pos_label=1)\n",
    "    f1 = f1_score(y_test, pred_b, pos_label=1)\n",
    "    auc = roc_auc_score(y_test, pred)\n",
    "\n",
    "    return acc, pre, re, f1, auc\n",
    "\n",
    "def print_clf_eval(y_test, pred):\n",
    "    pred_b = (pred > 0.5)\n",
    "    confusion = confusion_matrix(y_test, pred_b)\n",
    "    acc, pre, re, f1, auc = get_clf_eval(y_test, pred)\n",
    "\n",
    "    print(\"=Confusion matrix=\")\n",
    "    print(confusion)\n",
    "    print(\"==================\")\n",
    "\n",
    "    print(f\"Acc : {acc:.4f}, Pre : {pre:.4f}\")\n",
    "    print(f\"Re : {re:.4f}, F1 : {f1:.4f}, AUC : {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wafer_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 소개\n",
    "- 해당 데이터셋은 반도체 불량 검출을 위한 데이터셋으로 반도체 불량 여부가 class(0 or 1)로 포함되어있다.\n",
    "- 반도체 공정의 기밀 유지를 위해 모든 feature의 이름은 삭제되어있다.\n",
    "- feature_1~3 : numeric feature\n",
    "- feature_4~1558 : binary feature\n",
    "- class : binary class (0 : 양품, 1 : 불량)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *EDA는 생략되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, Y 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Class'], axis=1)\n",
    "Y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set, Test set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=777, stratify=Y)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "Y_train.reset_index(drop=True, inplace=True)\n",
    "Y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분리 확인\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y 밸런스 확인\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 전처리 조합 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.history = {'loss' : [], 'binary_accuracy' : [], 'val_loss' : [], 'val_binary_accuracy' : []}\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.history['loss'].append(logs['loss'])\n",
    "        self.history['binary_accuracy'].append(logs['binary_accuracy'])\n",
    "        self.history['val_loss'].append(logs['val_loss'])\n",
    "        self.history['val_binary_accuracy'].append(logs['val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_log(X_train, Y_train, X_test, Y_test, class_weight, l2, patience, fold):\n",
    "    AUCScores_log = np.zeros(10)\n",
    "    model_log = [0 for _ in range(10)]\n",
    "    for i in range(10, 101, 10):\n",
    "        tf.random.set_seed(i)\n",
    "        model_log[int(i/10 - 1)] = models.Sequential(name=f\"Keras_log_Random_state_{i}\")\n",
    "        model_log[int(i/10 - 1)].add(layers.Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],), \\\n",
    "                                  kernel_regularizer=regularizers.l2(l2)))\n",
    "        model_log[int(i/10 - 1)].compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=fold)\n",
    "        modelHistory = ModelHistory()\n",
    "        cb = [callbacks.EarlyStopping(monitor='val_loss', patience=patience, min_delta=0.01), \\\n",
    "             modelHistory]\n",
    "        \n",
    "        for train_index, test_index in kfold.split(X_train, Y_train):\n",
    "            X_train_log, X_val_log = X_train.loc[train_index], X_train.loc[test_index]\n",
    "            Y_train_log, Y_val_log = Y_train.loc[train_index], Y_train.loc[test_index]\n",
    "            model_log[int(i/10 - 1)].fit(X_train_log, Y_train_log, validation_data=(X_val_log, Y_val_log), \\\n",
    "                          epochs=50, use_multiprocessing=True, workers=-1, verbose=0, \\\n",
    "                          callbacks=cb, class_weight=class_weight)\n",
    "\n",
    "        model_log_pred = model_log[int(i/10 - 1)].predict(X_test)\n",
    "        AUCScores_log[int(i/10 - 1)] = roc_auc_score(Y_test, model_log_pred)\n",
    "\n",
    "    return AUCScores_log.mean().round(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_learning(data):\n",
    "    toScale = [\"feature_1\", \"feature_2\" , \"feature_3\"]\n",
    "    iqr_fold = [1, 1.25, 1.5, 3]\n",
    "    #iqr_fold.reverse()\n",
    "    corr_threshold = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    #corr_threshold.reverse()\n",
    "    stdScaler = StandardScaler()\n",
    "    mmScaler = MinMaxScaler()\n",
    "    robustScaler = RobustScaler()\n",
    "    maxabsScaler = MaxAbsScaler()\n",
    "    scalers = [stdScaler, mmScaler, robustScaler, maxabsScaler]\n",
    "    #scalers.reverse()\n",
    "    smote = SMOTE(random_state=0, k_neighbors=3)\n",
    "    randomOver = RandomOverSampler(random_state=0)\n",
    "    borderSmote = BorderlineSMOTE(random_state=0, k_neighbors=3)\n",
    "    adasyn = ADASYN(random_state=0, n_jobs=-1)\n",
    "    oversamplers = ['weight', smote, randomOver, borderSmote, adasyn]\n",
    "    #oversamplers.reverse()\n",
    "    l2s = [0.001, 0.0001]\n",
    "    #l2s.reverse()\n",
    "    patiences = [10, 50]\n",
    "    #patiences.reverse()\n",
    "    folds = [5, 10]\n",
    "    #folds.reverse()\n",
    "    \n",
    "    X = data.drop(['Class'], axis=1)\n",
    "    Y = data['Class']\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=777, stratify=Y)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    Y_train.reset_index(drop=True, inplace=True)\n",
    "    Y_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for i in X_train.columns:\n",
    "        if len(X_train[i].unique()) == 1:\n",
    "            X_train.drop(i, axis=1, inplace=True)\n",
    "            X_test.drop(i, axis=1, inplace=True)\n",
    "    \n",
    "    for dup in [True]:\n",
    "        for iqr in iqr_fold:\n",
    "            trimmer = OutlierTrimmer(capping_method='iqr', tail='both', fold=iqr, variables=toScale)\n",
    "            X_train_iqr = trimmer.fit_transform(X_train)\n",
    "            Y_train_iqr = Y_train[X_train_iqr.index]\n",
    "            X_test_iqr = X_test.copy()\n",
    "            Y_test_iqr = Y_test.copy()\n",
    "            X_train_iqr.reset_index(drop=True, inplace=True)\n",
    "            Y_train_iqr.reset_index(drop=True, inplace=True)\n",
    "            X_test_iqr.reset_index(drop=True, inplace=True)\n",
    "            Y_test_iqr.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            for corr_th in corr_threshold:\n",
    "                corr = []\n",
    "                for i in X_train_iqr.columns:\n",
    "                    corr.append(abs(np.corrcoef(X_train_iqr[i], Y_train_iqr)[0][1]))\n",
    "                corrX_Y = pd.DataFrame(columns=['features', 'corr'])\n",
    "                corrX_Y['features'] = X_train_iqr.columns\n",
    "                corrX_Y['corr'] = corr\n",
    "                corr_mat = X_train_iqr.corr()\n",
    "                corr_mat = abs(corr_mat)\n",
    "                overCorr = pd.DataFrame(columns=['INDEX', 'COLUMN', 'CORR'])\n",
    "                INDEX, COLUMN, CORR = [], [], []\n",
    "                for i in range(len(corr_mat.index)):\n",
    "                    for j in range(i+1, len(corr_mat.columns)):\n",
    "                        if corr_mat.iloc[i, j] > corr_th:\n",
    "                            #print(corr_mat.index[i], corr_mat.columns[j], corr_mat.iloc[i, j])\n",
    "                            INDEX.append(corr_mat.index[i])\n",
    "                            COLUMN.append(corr_mat.columns[j])\n",
    "                            CORR.append(corr_mat.iloc[i, j])\n",
    "                overCorr['INDEX'] = INDEX\n",
    "                overCorr['COLUMN'] = COLUMN\n",
    "                overCorr['CORR'] = CORR\n",
    "                for i in overCorr.index:\n",
    "                    try:\n",
    "                        if corrX_Y[corrX_Y['features'] == overCorr.loc[i]['INDEX']]['corr'].values[0] <= \\\n",
    "                        corrX_Y[corrX_Y['features'] == overCorr.loc[i]['COLUMN']]['corr'].values[0]:\n",
    "                            X_train_corr = X_train_iqr.drop([overCorr.loc[i]['INDEX']], axis=1)\n",
    "                            X_test_corr = X_test_iqr.drop([overCorr.loc[i]['INDEX']], axis=1)\n",
    "                        else:\n",
    "                            X_train_corr = X_train_iqr.drop([overCorr.loc[i]['COLUMN']], axis=1)\n",
    "                            X_test_corr = X_test_iqr.drop([overCorr.loc[i]['COLUMN']], axis=1)\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                for scaler in scalers:\n",
    "                    toScale = []\n",
    "                    for i in X_train_corr.columns:\n",
    "                        if i == 'feature_1':\n",
    "                            toScale.append(i)\n",
    "                            continue\n",
    "                        elif i == 'feature_2':\n",
    "                            toScale.append(i)\n",
    "                            continue\n",
    "                        elif i == 'feature_3':\n",
    "                            toScale.append(i)\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "                    X_train_scale = X_train_corr.copy()\n",
    "                    X_test_scale = X_test_corr.copy()\n",
    "                    for feature in toScale:\n",
    "                        X_train_scale.loc[:, feature] = scaler.fit_transform(X_train_corr[feature].to_numpy().reshape(-1, 1))\n",
    "                        X_test_scale.loc[:, feature] = scaler.transform(X_test_corr[feature].to_numpy().reshape(-1, 1))\n",
    "                            \n",
    "                    for over in oversamplers:\n",
    "                        for l2 in l2s:\n",
    "                            for patience in patiences:\n",
    "                                for fold in folds:\n",
    "                                    #print(f\"dup:{str(dup)}, iqr:{str(iqr)}, corr_th:{str(corr_th)}, scaler:{str(scaler)}\")                         \n",
    "                                    X_train_over, Y_train_over = X_train_scale.copy(), Y_train_iqr.copy()\n",
    "                                    X_test_over, Y_test_over = X_test_scale.copy(), Y_test_iqr.copy()\n",
    "                                    if over != 'weight':\n",
    "                                        X_train_over, Y_train_over = over.fit_resample(X_train_scale, Y_train_over)\n",
    "\n",
    "                                    for i in X_train_over.columns:\n",
    "                                        if len(X_train_over[i].unique()) == 1:\n",
    "                                            X_train_over.drop(i, axis=1, inplace=True)\n",
    "                                            X_test_over.drop(i, axis=1, inplace=True)\n",
    "\n",
    "                                    weight_0 = (1 / Y_train_over.value_counts()[0]) * (Y_train_over.value_counts().sum() / 2.0)\n",
    "                                    weight_1 = (1 / Y_train_over.value_counts()[1]) * (Y_train_over.value_counts().sum() / 2.0)\n",
    "                                    class_weight = {0 : weight_0, 1 : weight_1}\n",
    "\n",
    "                                    score = keras_log(X_train_over, Y_train_over, X_test_over, Y_test_over, class_weight, l2, patience, fold)\n",
    "                                    result = f\"dup:{str(dup)}, iqr:{str(iqr)}, corr_th:{str(corr_th)}, scaler:{str(scaler).split('Scaler')[0]}, oversampler:{str(over).split('(')[0]}, l2:{str(l2)}, patience:{str(patience)}, fold:{str(fold)}, score:{str(score)}\"\n",
    "                                    print(result)\n",
    "\n",
    "                                    with open(\"infinite_learning.txt\", \"a\") as f:\n",
    "                                        f.write(result+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wafer_data.csv')\n",
    "infinite_learning(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중복 레코드 제거\n",
    "- 특정 레코드에 과적합되는 것을 방지하기위해 중복 레코드 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([X_train, Y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = temp['Class']\n",
    "X_train = temp.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "Y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "Y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단일 항목을 가진 feature 제거\n",
    "- 단일 항목을 가진 feature는 학습에 영향을 미치지 않고 추후 다른 항목을 가진 data를 예측할 때 정확도가 떨어질 수 있기 때문에 해당 feature는 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_train.columns:\n",
    "    if len(X_train[i].unique()) == 1:\n",
    "        X_train.drop(i, axis=1, inplace=True)\n",
    "        X_test.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감소한 피쳐 수 확인\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier 처리 #0 안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toScale = [\"feature_1\", \"feature_2\", \"feature_3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier 처리 #1 iqr 기준 삭제\n",
    "- Train set feature_1~3에 대해 IQR 범위 바깥의 이상치들을 제거함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toScale = [\"feature_1\", \"feature_2\" , \"feature_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = OutlierTrimmer(capping_method='iqr', tail='both', fold=3, variables=toScale)\n",
    "X_train = trimmer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train[X_train.index]\n",
    "Y_test = Y_test[X_test.index]\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "Y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "Y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature간 상관계수 계산 후 제거\n",
    "- 각 feature와 class 간 피어슨 상관계수를 계산하여 데이터프레임에 저장한다.\n",
    "- 이후 각 feature 간 피어슨 상관계수를 계산하고 threshold를 넘는 상관관계를 가진 두 feature의 이름과 상관계수를 데이터프레임에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = []\n",
    "\n",
    "for i in X_train.columns:\n",
    "    corr.append(abs(np.corrcoef(X_train[i], Y_train)[0][1]))\n",
    "corrX_Y = pd.DataFrame(columns=['features', 'corr'])\n",
    "corrX_Y['features'] = X_train.columns\n",
    "corrX_Y['corr'] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = abs(corr_mat)\n",
    "#corr_mat.to_csv('./corr_mat.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overCorr = pd.DataFrame(columns=['INDEX', 'COLUMN', 'CORR'])\n",
    "INDEX, COLUMN, CORR = [], [], []\n",
    "for i in range(len(corr_mat.index)):\n",
    "    for j in range(i+1, len(corr_mat.columns)):\n",
    "        if corr_mat.iloc[i, j] > 0.7:\n",
    "            #print(corr_mat.index[i], corr_mat.columns[j], corr_mat.iloc[i, j])\n",
    "            INDEX.append(corr_mat.index[i])\n",
    "            COLUMN.append(corr_mat.columns[j])\n",
    "            CORR.append(corr_mat.iloc[i, j])\n",
    "overCorr['INDEX'] = INDEX\n",
    "overCorr['COLUMN'] = COLUMN\n",
    "overCorr['CORR'] = CORR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overCorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature간 상관계수 계산 후 제거\n",
    "- 위에서 계산한 데이터프레임을 바탕으로 두 feature 중 class와의 상관계수가 낮은 feature를 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in overCorr.index:\n",
    "    try:\n",
    "        if corrX_Y[corrX_Y['features'] == overCorr.loc[i]['INDEX']]['corr'].values[0] <= \\\n",
    "        corrX_Y[corrX_Y['features'] == overCorr.loc[i]['COLUMN']]['corr'].values[0]:\n",
    "            X_train.drop([overCorr.loc[i]['INDEX']], axis=1, inplace=True)\n",
    "            X_test.drop([overCorr.loc[i]['INDEX']], axis=1, inplace=True)\n",
    "        else:\n",
    "            X_train.drop([overCorr.loc[i]['COLUMN']], axis=1, inplace=True)\n",
    "            X_test.drop([overCorr.loc[i]['COLUMN']], axis=1, inplace=True)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "- Numeric feature인 feature_1~3의 단위를 맞추고 이상치 영향을 줄이기 위해 다양한 scale 방법을 시도한다.\n",
    "- Test set은 train set을 scale 한 기준을 가지고 scale한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling #0 안함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling #1 StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "for feature in toScale:\n",
    "    X_train.loc[:, feature] = scaler.fit_transform(X_train[feature].to_numpy().reshape(-1, 1))\n",
    "    X_test.loc[:, feature] = scaler.transform(X_test[feature].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling #2 MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mmScaler = MinMaxScaler()\n",
    "for feature in toScale:\n",
    "    X_train.loc[:, feature] = mmScaler.fit_transform(X_train[feature].to_numpy().reshape(-1, 1))\n",
    "    X_test.loc[:, feature] = mmScaler.transform(X_test[feature].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling #3 RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "robustScaler = RobustScaler()\n",
    "for feature in toScale:\n",
    "    X_train.loc[:, feature] = robustScaler.fit_transform(X_train[feature].to_numpy().reshape(-1, 1))\n",
    "    X_test.loc[:, feature] = robustScaler.transform(X_test[feature].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling #4 MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxabsScaler = MaxAbsScaler()\n",
    "for feature in toScale:\n",
    "    X_train.loc[:, feature] = maxabsScaler.fit_transform(X_train[feature].to_numpy().reshape(-1, 1))\n",
    "    X_test.loc[:, feature] = maxabsScaler.transform(X_test[feature].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클래스 불균형\n",
    "- 이 데이터셋은 극심한 클래스 불균형을 이루고 있어 그대로 학습하면 모델이 올바르게 학습하지 못하는 결과를 낳는다.\n",
    "- 따라서 오버샘플링이나 언더샘플링, class_weight를 사용하는 기법을 통해 클래스의 불균형을 해소할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버샘플링 #1 SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=2, k_neighbors=3)\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버샘플링 #2 RamdomOverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomOver = RandomOverSampler(random_state=0)\n",
    "X_train, Y_train = randomOver.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버샘플링 #3 SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smotenc = SMOTENC(random_state=2, categorical_features=range(3, X_train.shape[1]),\\\n",
    "                  k_neighbors=3, n_jobs=-1)\n",
    "X_train, Y_train = smotenc.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버샘플링 #4 BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "borderSmote = BorderlineSMOTE(random_state=2, k_neighbors=3)\n",
    "X_train, Y_train = borderSmote.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버샘플링 #5 ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adasyn = ADASYN(random_state=2, n_jobs=-1)\n",
    "X_train, Y_train = adasyn.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y 밸런스 확인\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오버샘플링 진행 후 단일 클래스 가진 피쳐 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_train.columns:\n",
    "    if len(X_train[i].unique()) == 1:\n",
    "        X_train.drop(i, axis=1, inplace=True)\n",
    "        X_test.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감소한 피쳐 수 확인\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오버샘플링 진행 후 중복 데이터 제거\n",
    "#X_train.drop_duplicates(keep='first', inplace=True)\n",
    "#Y_train = Y_train.loc[X_train.index]\n",
    "#X_train.reset_index(drop=True, inplace=True)\n",
    "#Y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y 밸런스 확인\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class_weight\n",
    "- 클래스 불균형을 해소하기위한 기법 중 하나로 클래스 비율에 따라 모델 손실 함수에 가중치를 부여하여 모델이 소수 클래스에 더 많은 '관심'을 갖도록 한다.\n",
    "- 클래스 비율을 구하는 공식은 구글 텐서플로우 공식 레퍼런스에 소개되어 있다.\n",
    "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_0 = (1 / Y_train.value_counts()[0]) * (Y_train.value_counts().sum() / 2.0)\n",
    "weight_1 = (1 / Y_train.value_counts()[1]) * (Y_train.value_counts().sum() / 2.0)\n",
    "class_weight = {0 : weight_0, 1 : weight_1}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model history callback class\n",
    "- 케라스 모델 학습 시 한 epoch가 끝날 때 계산된 loss와 accuracy를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.history = {'loss' : [], 'binary_accuracy' : [], 'val_loss' : [], 'val_binary_accuracy' : []}\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.history['loss'].append(logs['loss'])\n",
    "        self.history['binary_accuracy'].append(logs['binary_accuracy'])\n",
    "        self.history['val_loss'].append(logs['val_loss'])\n",
    "        self.history['val_binary_accuracy'].append(logs['val_binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "- 모든 모델 학습 시 random_state 또는 seed를 10 ~ 100 까지 10 단위로 10 개의 모델을 학습하고 각 모델의 ROC_AUC_SCORE를 계산하여 평균을 구하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCScores_log = np.zeros(10)\n",
    "model_log = [0 for _ in range(10)]\n",
    "for i in range(10, 101, 10):\n",
    "    tf.random.set_seed(i)\n",
    "    model_log[int(i/10 - 1)] = models.Sequential(name=f\"Keras_log_Random_state_{i}\")\n",
    "    model_log[int(i/10 - 1)].add(layers.Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],), \\\n",
    "                              kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model_log[int(i/10 - 1)].compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    modelHistory = ModelHistory()\n",
    "    cb = [callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=0.01), \\\n",
    "         modelHistory]\n",
    "    with tqdm(total=5, ascii=True) as pbar:\n",
    "        for train_index, test_index in kfold.split(X_train, Y_train):\n",
    "            X_train_log, X_val_log = X_train.loc[train_index], X_train.loc[test_index]\n",
    "            Y_train_log, Y_val_log = Y_train.loc[train_index], Y_train.loc[test_index]\n",
    "            model_log[int(i/10 - 1)].fit(X_train_log, Y_train_log, validation_data=(X_val_log, Y_val_log), \\\n",
    "                          epochs=50, use_multiprocessing=True, workers=-1, verbose=0, \\\n",
    "                          callbacks=cb, class_weight=class_weight)\n",
    "            pbar.update(1)\n",
    "\n",
    "    model_log_pred = model_log[int(i/10 - 1)].predict(X_test)\n",
    "    AUCScores_log[int(i/10 - 1)] = roc_auc_score(Y_test, model_log_pred)\n",
    "    #print_clf_eval(Y_test, model_log_pred)\n",
    "\n",
    "print(AUCScores_log.mean().round(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(modelHistory.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(modelHistory.history['val_loss'], 'g', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(modelHistory.history['binary_accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(modelHistory.history['val_binary_accuracy'], 'r', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCScores_mlp = np.zeros(10)\n",
    "model_mlp = [0 for _ in range(10)]\n",
    "for i in range(10, 101, 10):\n",
    "    tf.random.set_seed(i)\n",
    "    model_mlp[int(i/10 - 1)] = models.Sequential(name=f\"Keras_mlp_Random_state_{i}\")\n",
    "    model_mlp[int(i/10 - 1)].add(layers.Dense(100, activation='relu', input_shape=(X_train.shape[1],), \\\n",
    "                               kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    #model_mlp[int(i/10 - 1)].add(layers.Dropout(0.5))\n",
    "    model_mlp[int(i/10 - 1)].add(layers.Dense(1, activation='sigmoid'))\n",
    "    model_mlp[int(i/10 - 1)].compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    modelHistory = ModelHistory()\n",
    "    cb = [callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=0.01), \\\n",
    "         modelHistory]\n",
    "    with tqdm(total=5, ascii=True) as pbar:\n",
    "        for train_index, test_index in kfold.split(X_train, Y_train):\n",
    "            X_train_mlp, X_val_mlp = X_train.loc[train_index], X_train.loc[test_index]\n",
    "            Y_train_mlp, Y_val_mlp = Y_train.loc[train_index], Y_train.loc[test_index]\n",
    "            model_mlp[int(i/10 - 1)].fit(X_train_mlp, Y_train_mlp, validation_data=(X_val_mlp, Y_val_mlp), \\\n",
    "                          epochs=50, use_multiprocessing=True, workers=-1, verbose=0, \\\n",
    "                          callbacks=cb, class_weight=class_weight)\n",
    "            pbar.update(1)\n",
    "\n",
    "    model_mlp_pred = model_mlp[int(i/10 - 1)].predict(X_test)\n",
    "    AUCScores_mlp[int(i/10 - 1)] = roc_auc_score(Y_test, model_mlp_pred)\n",
    "    #print_clf_eval(Y_test, model_mlp_pred)\n",
    "        \n",
    "print(AUCScores_mlp.mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(modelHistory.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(modelHistory.history['val_loss'], 'g', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(modelHistory.history['binary_accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(modelHistory.history['val_binary_accuracy'], 'r', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCScores_dnn = np.zeros(10)\n",
    "model_dnn = [0 for _ in range(10)]\n",
    "for i in range(10, 101, 10):\n",
    "    tf.random.set_seed(i)\n",
    "    model_dnn[int(i/10 - 1)] = models.Sequential()\n",
    "    model_dnn[int(i/10 - 1)].add(layers.Dense(100, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model_dnn[int(i/10 - 1)].add(layers.Dense(100, activation='relu'))\n",
    "    model_dnn[int(i/10 - 1)].add(layers.Dense(100, activation='relu'))\n",
    "    model_dnn[int(i/10 - 1)].add(layers.Dense(100, activation='relu'))\n",
    "    model_dnn[int(i/10 - 1)].add(layers.Dense(100, activation='relu'))\n",
    "    model_dnn[int(i/10 - 1)].add(layers.Dense(100, activation='relu'))\n",
    "    model_dnn[int(i/10 - 1)].add(layers.Dense(100, activation='relu'))\n",
    "    model_dnn[int(i/10 - 1)].add(layers.Dense(1, activation='sigmoid'))\n",
    "    model_dnn[int(i/10 - 1)].compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    modelHistory = ModelHistory()\n",
    "    cb = [callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=0.01), \\\n",
    "         modelHistory]\n",
    "    with tqdm(total=5, ascii=True) as pbar:\n",
    "        for train_index, test_index in kfold.split(X_train, Y_train):\n",
    "            X_train_dnn, X_val_dnn = X_train.loc[train_index], X_train.loc[test_index]\n",
    "            Y_train_dnn, Y_val_dnn = Y_train.loc[train_index], Y_train.loc[test_index]\n",
    "            model_dnn[int(i/10 - 1)].fit(X_train_dnn, Y_train_dnn, validation_data=(X_val_dnn, Y_val_dnn), \\\n",
    "                          epochs=50, use_multiprocessing=True, workers=-1, verbose=0, \\\n",
    "                          callbacks=cb, class_weight=class_weight)\n",
    "            pbar.update(1)\n",
    "\n",
    "    model_dnn_pred = model_dnn[int(i/10 - 1)].predict(X_test)\n",
    "    AUCScores_dnn[int(i/10 - 1)] = roc_auc_score(Y_test, model_dnn_pred)\n",
    "    #print_clf_eval(Y_test, model_dnn_pred)\n",
    "\n",
    "print(AUCScores_dnn.mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(modelHistory.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(modelHistory.history['val_loss'], 'g', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(modelHistory.history['binary_accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(modelHistory.history['val_binary_accuracy'], 'r', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCScores_sk_log = np.zeros(10)\n",
    "sk_log = [0 for _ in range(10)]\n",
    "for i in range(10, 101, 10):\n",
    "    sk_log[int(i / 10 - 1)] = LogisticRegression(random_state=i, solver='liblinear', class_weight=class_weight)\n",
    "    sk_log[int(i / 10 - 1)].fit(X_train, Y_train)\n",
    "    sk_log_pred = sk_log[int(i / 10 - 1)].predict(X_test)\n",
    "    AUCScores_sk_log[int(i/10 - 1)] = roc_auc_score(Y_test, sk_log_pred)\n",
    "    #print_clf_eval(Y_test, sk_log_pred)\n",
    "\n",
    "print(AUCScores_sk_log.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCScores_sk_tree = np.zeros(10)\n",
    "sk_tree = [0 for _ in range(10)]\n",
    "for i in range(10, 101, 10):\n",
    "    sk_tree[int(i/10 - 1)] = DecisionTreeClassifier(random_state=i, max_depth=30, class_weight=class_weight)\n",
    "    sk_tree[int(i/10 - 1)].fit(X_train, Y_train)\n",
    "    sk_tree_pred = sk_tree[int(i/10 - 1)].predict(X_test)\n",
    "    AUCScores_sk_tree[int(i/10 - 1)] = roc_auc_score(Y_test, sk_tree_pred)\n",
    "    #print_clf_eval(Y_test, sk_tree_pred)\n",
    "    \n",
    "print(AUCScores_sk_tree.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCScores_sk_rf = np.zeros(10)\n",
    "sk_rf = [0 for _ in range(10)]\n",
    "for i in range(10, 101, 10):\n",
    "    sk_rf[int(i/10 - 1)] = RandomForestClassifier(random_state=i, n_jobs=-1, n_estimators=10, class_weight=class_weight)\n",
    "    sk_rf[int(i/10 - 1)].fit(X_train, Y_train)\n",
    "    sk_rf_pred = sk_rf[int(i/10 -1 )].predict(X_test)\n",
    "    AUCScores_sk_rf[int(i/10 - 1)] = roc_auc_score(Y_test, sk_rf_pred)\n",
    "    #print_clf_eval(Y_test, sk_rf_pred)\n",
    "\n",
    "print(AUCScores_sk_rf.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCScores_lgbm = np.zeros(10)\n",
    "lgbm = [0 for _ in range(10)]\n",
    "for i in range(10, 101, 10):\n",
    "    lgbm[int(i/10 - 1)] = LGBMClassifier(n_estimators=300, num_leaves=100, n_jobs=-1, \\\n",
    "                          boost_from_average=False, random_state=i, class_weight=class_weight)\n",
    "    lgbm[int(i/10 - 1)].fit(X_train, Y_train)\n",
    "    lgbm_pred = lgbm[int(i/10 - 1)].predict(X_test)\n",
    "    AUCScores_lgbm[int(i/10 - 1)] = roc_auc_score(Y_test, lgbm_pred)\n",
    "    #print_clf_eval(Y_test, lgbm_pred)\n",
    "\n",
    "print(AUCScores_lgbm.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AUCScores_xgb = np.zeros(10)\n",
    "xgb = [0 for _ in range(10)]\n",
    "for i in range(10, 101, 10):\n",
    "    xgb[int(i/10 - 1)] = XGBClassifier(random_state=i, eta=0.2, max_depth=0, min_child_weight=1, \\\n",
    "                                       n_estimators=500, objective='binary:logistic', scale_pos_weight=Y_train.value_counts()[1]/Y_train.value_counts()[0])\n",
    "    xgb[int(i/10 - 1)].fit(X_train, Y_train)\n",
    "    xgb_pred = xgb[int(i/10 -1)].predict(X_test)\n",
    "    AUCScores_xgb[int(i/10 - 1)] = roc_auc_score(Y_test, xgb_pred)\n",
    "    #print_clf_eval(Y_test, x_pred)\n",
    "\n",
    "print(AUCScores_xgb.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indexs = ['Keras MLP', 'Keras DNN', 'Keras logistic regression', 'Sklearn logistic regression', 'Sklearn Decision Tree', 'Sklearn RandomForest', 'LightGBM', 'XGBoost']\n",
    "columns = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC-AUC']\n",
    "temp = []\n",
    "\n",
    "temp.append(get_clf_eval(Y_test, model_mlp_pred))\n",
    "temp.append(get_clf_eval(Y_test, model_dnn_pred))\n",
    "temp.append(get_clf_eval(Y_test, model_log_pred))\n",
    "temp.append(get_clf_eval(Y_test, sk_log_pred))\n",
    "temp.append(get_clf_eval(Y_test, sk_tree_pred))\n",
    "temp.append(get_clf_eval(Y_test, sk_rf_pred))\n",
    "temp.append(get_clf_eval(Y_test, lgbm_pred))\n",
    "temp.append(get_clf_eval(Y_test, xgb_pred))\n",
    "\n",
    "results = pd.DataFrame(temp, index=indexs, columns=columns)\n",
    "results.loc['Keras logistic regression', 'ROC-AUC'] = AUCScores_log.mean().round(8)\n",
    "results.loc['Keras MLP', 'ROC-AUC'] = AUCScores_mlp.mean().round(8)\n",
    "results.loc['Keras DNN', 'ROC-AUC'] = AUCScores_dnn.mean().round(8)\n",
    "results.loc['Sklearn logistic regression', 'ROC-AUC'] = AUCScores_sk_log.mean().round(8)\n",
    "results.loc['Sklearn Decision Tree', 'ROC-AUC'] = AUCScores_sk_tree.mean().round(8)\n",
    "results.loc['Sklearn RandomForest', 'ROC-AUC'] = AUCScores_sk_rf.mean().round(8)\n",
    "results.loc['LightGBM', 'ROC-AUC'] = AUCScores_lgbm.mean().round(8)\n",
    "results.loc['XGBoost', 'ROC-AUC'] = AUCScores_xgb.mean().round(8)\n",
    "\n",
    "results.sort_values(by=['ROC-AUC'], axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
